{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing and transforming raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import json\n",
    "import dotenv\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract ArXiv abstracts from local paper directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from pypdf import PdfReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_paper_directory = \"\" # your directory here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "during clean up, i had the need to rename paper files with their arxiv id. to do this, i parse the first page and filter for arxiv id format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(arxiv_paper_directory):\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        filepath = os.path.join(arxiv_paper_directory, filename)\n",
    "        with open(filepath, 'rb') as pdf_file:\n",
    "            pdf_reader = PdfReader(pdf_file)\n",
    "            first_page = pdf_reader.pages[0]\n",
    "            text = first_page.extract_text()\n",
    "\n",
    "            # Search for the arXiv ID pattern on the first page\n",
    "            match = re.search(r'(\\d{4}\\.\\d{5}v?\\d?)', text)\n",
    "            if match:\n",
    "                arxiv_id = match.group(1)\n",
    "                # Remove the \"v\" and digit from the arXiv ID\n",
    "                arxiv_id = re.sub(r'v\\d', '', arxiv_id)\n",
    "                new_filename = f\"{arxiv_id}.pdf\"\n",
    "                new_filepath = os.path.join(arxiv_paper_directory, new_filename)\n",
    "                os.rename(filepath, new_filepath)\n",
    "                print(f\"Renamed: {filename} -> {new_filename}\")\n",
    "            else:\n",
    "                print(f\"No arXiv ID found for: {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fetch abstracts using ArXiv api and save to jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "output_filename = \"~/arxiv_paper_abs.jsonl\"\n",
    "jsonl_filepath = os.path.expanduser(output_filename)\n",
    "\n",
    "with open(jsonl_filepath, \"w\") as jsonl_file:\n",
    "    for filename in tqdm(os.listdir(arxiv_paper_directory)):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            arxiv_id = filename.replace(\".pdf\", \"\")\n",
    "            url = f\"http://export.arxiv.org/api/query?id_list={arxiv_id}\"\n",
    "            response = requests.get(url)\n",
    "            if response.status_code == 200:\n",
    "                feed = response.content.decode(\"utf-8\")\n",
    "                start_index = feed.find(\"<summary>\")\n",
    "                end_index = feed.find(\"</summary>\")\n",
    "                if start_index != -1 and end_index != -1:\n",
    "                    abstract = (\n",
    "                        feed[start_index + 9 : end_index].strip().replace(\"\\n\", \" \")\n",
    "                    )\n",
    "                    data = {\"id\": arxiv_id, \"abstract\": abstract}\n",
    "                    jsonl_file.write(json.dumps(data) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract doi ids from local paper directory\n",
    "\n",
    "This assumes that the file names contain valid doi ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doi_paper_directory = \"\" # put the directory path to the doi documents here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "find and rename those with a doi id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in tqdm(os.listdir(doi_paper_directory)):\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        filepath = os.path.join(doi_paper_directory, filename)\n",
    "        with open(filepath, 'rb') as pdf_file:\n",
    "            pdf_reader = PdfReader(pdf_file)\n",
    "            first_page = pdf_reader.pages[0]\n",
    "            text = first_page.extract_text() if first_page else \"\"\n",
    "\n",
    "            # Regex pattern for matching a DOI link\n",
    "            doi_pattern = r'https?://doi\\.org/10\\.[0-9]+/[^\\s]+'\n",
    "            match = re.search(doi_pattern, text)\n",
    "            if match:\n",
    "                doi_link = match.group(0)\n",
    "                # Extract the DOI identifier and replace slashes with dots, \n",
    "                # we do this so the saving path is not messed up\n",
    "                doi_identifier = doi_link.replace('https://doi.org/', '').replace('/', '.')\n",
    "                new_filename = f\"{doi_identifier}.pdf\"\n",
    "                new_filepath = os.path.join(doi_paper_directory, new_filename)\n",
    "                os.rename(filepath, new_filepath)\n",
    "                print(f\"Renamed: {filename} -> {new_filename}\")\n",
    "            else:\n",
    "                print(f\"No DOI link found for: {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extract the doi ids from filename again. this seems redundant, it was there because i first had to rename the files for my own sanity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "doi_ids = []\n",
    "doi_pattern = re.compile(r\"^\\d{2}\\.\\d{4,}\\..*?(?:doi:)?\\.pdf$\")\n",
    "\n",
    "for filename in os.listdir(doi_paper_directory):\n",
    "    if doi_pattern.match(filename):\n",
    "        doi_id = filename[:-4]\n",
    "        first_period_idx = doi_id.find(\".\")\n",
    "        doi_id = doi_id[: first_period_idx + 1] + doi_id[\n",
    "            first_period_idx + 1 :\n",
    "        ].replace(\".\", \"/\", 1)\n",
    "        doi_ids.append(doi_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected DOI IDs:\n",
      "59\n",
      "['10.1016/j.cell.2023.12.034', '10.1101/2024.01.02.573943', '10.1016/j.acha.2021.12.009', '10.1016/j.cell.2023.12.035', '10.1016/j.cell.2024.01.026', '10.1038/s41467-021-26529-9', '10.1016/j.cell.2023.12.037', '10.1093/gbe.evad084', '10.1038/s41467-024-46631-y', '10.1093/molbev.msx095', '10.1016/j.cell.2023.12.026', '10.1016/j.cell.2023.12.032', '10.1101/2023.04.30.538439', '10.1038/s41586-019-1923-7', '10.1101/2024.03.21.585615', '10.1038/s41586-023-06291-2', '10.1038/s41467-024-46715-9', '10.1101/2021.02.12.430858', '10.1038/s41586-019-1724-z', '10.1016/j.cell.2024.01.036', '10.1038/s41467-023-38539-w', '10.1126/science.abo7201', '10.1038/s41467-021-25756-4', '10.1038/s41564-023-01584-8', '10.7554/eLife.50524.001', '10.1126/science.aay8015', '10.1038/s42004-024-01098-2', '10.1101/2024.02.06.579080', '10.1016/j.bpj.2017.10.028', '10.1038/s41588-023-01649-8', '10.1101/2024.03.07.584001', '10.1145/3600006.3613165', '10.1016/j.cell.2023.04.032', '10.1038/s41593-023-01304-9', '10.1101/2022.05.17.492325', '10.1101/2020.12.15.422761', '10.1038/s41586-024-07177-7', '10.1101/2022.11.18.517004', '10.1038/s41586-023-06832-9', '10.1038/s41467-023-37023-9', '10.1038/s41586-023-06924-6', '10.1101/2021.07.09.450648', '10.1101/2023.09.11.556673', '10.1016/j.cell.2023.12.028', '10.1016/j.cels.2024.01.008', '10.1126/science.abm9326', '10.1038/s41586-024-07196-4', '10.1016/j.cell.2024.01.005', '10.1101/2022.12.21.521526', '10.1016/j.cell.2023.12.016', '10.1038/s41586-024-07128-2', '10.1016/j.cell.2023.12.017', '10.1038/s41586-021-03819-2', '10.1016/j.cell.2023.12.012', '10.2307/2334029', '10.1016/j.cell.2024.01.003', '10.1016/j.cell.2023.12.010', '10.1101/2024.02.29.582810', '10.1101/2022.12.21.521521']\n"
     ]
    }
   ],
   "source": [
    "print(\"Collected DOI IDs:\")\n",
    "print(len(doi_ids))\n",
    "print(doi_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can use crossref.org api to obtain metadata about each doi document. typically, we can look for the abstract info in the response's `message.abstract` field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:38<00:00,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOI IDs without abstract: ['10.1093/gbe.evad084', '10.1093/molbev.msx095', '10.1038/s41586-019-1923-7', '10.1038/s41586-019-1724-z', '10.1038/s41564-023-01584-8', '10.7554/eLife.50524.001', '10.1145/3600006.3613165', '10.1038/s41593-023-01304-9', '10.2307/2334029']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "elsevier_api_key = os.getenv(\"ELSEVIER_API_KEY\")\n",
    "\n",
    "save_dir = \"/Users/yxz/tp/datasets/papers/abstracts\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "doi_without_abstract_list = []\n",
    "\n",
    "with open(os.path.join(save_dir, \"doi_abstracts.jsonl\"), \"w\") as outfile:\n",
    "    for doi in tqdm(doi_ids):\n",
    "        url = f\"https://api.crossref.org/works/{doi}\"\n",
    "        response = requests.get(url)\n",
    "        abstract = None\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            if \"elsevier\" in data[\"message\"][\"publisher\"].lower():\n",
    "                headers = {\n",
    "                    \"X-ELS-APIKey\": elsevier_api_key,\n",
    "                    \"Accept\": \"application/json\",\n",
    "                }\n",
    "                url = f\"https://api.elsevier.com/content/article/doi/{doi}\"\n",
    "                response = requests.get(url, headers=headers)\n",
    "                if response.status_code == 200:\n",
    "                    data = response.json()\n",
    "                    abstract = (\n",
    "                        data.get(\"full-text-retrieval-response\", {})\n",
    "                        .get(\"coredata\", {})\n",
    "                        .get(\"dc:description\")\n",
    "                    )\n",
    "            elif \"abstract\" in data[\"message\"] and not abstract:\n",
    "                abstract = data[\"message\"][\"abstract\"]\n",
    "        if abstract:\n",
    "            # Elsevier returns a XML formatted abstract that contains both a title and paragraphs.\n",
    "            # We only want the paragraphs, so we use BeautifulSoup to parse the XML and extract the paragraphs.\n",
    "            soup = BeautifulSoup(abstract, \"html.parser\")\n",
    "            paragraphs = soup.find_all(\"jats:p\")\n",
    "            # Join the text of all paragraphs\n",
    "            abstract_text = \" \".join(p.text for p in paragraphs)\n",
    "            abstract_text = abstract_text.replace(\"\\n\", \" \").strip()\n",
    "            json.dump(\n",
    "                {\"doi_id\": doi, \"abstract\": abstract_text}, outfile, ensure_ascii=False\n",
    "            )\n",
    "            outfile.write(\"\\n\")\n",
    "        else:\n",
    "            doi_without_abstract_list.append(doi)\n",
    "\n",
    "print(f\"DOI IDs without abstract: {doi_without_abstract_list}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "manually examine the doi ids that are not found in the crossref api and save the response to a json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "id_to_fix = [\n",
    "    \"10.1093/gbe/evad084\",\n",
    "    \"10.1093/molbev/msx095\",\n",
    "    \"10.1038/s41586-019-1923-7\",\n",
    "    \"10.1038/s41586-019-1724-z\",\n",
    "    \"10.1038/s41564-023-01584-8\",\n",
    "    \"10.7554/eLife.50524.001\",\n",
    "    \"10.1145/3600006.3613165\",\n",
    "    \"10.1038/s41593-023-01304-9\",\n",
    "    \"10.2307/2334029\",\n",
    "]\n",
    "\n",
    "for doi_test in id_to_fix:\n",
    "    url_test = f\"https://api.crossref.org/works/{doi_test}\"\n",
    "    response_test = requests.get(url_test)\n",
    "\n",
    "    if response_test.status_code == 200:\n",
    "        data_test = response_test.json()\n",
    "        with open(f\"response_{doi_test.replace('/', '.')}.json\", \"w\") as f:\n",
    "            json.dump(data_test, f, ensure_ascii=False, indent=4)\n",
    "    else:\n",
    "        print(\"Failed to fetch data for DOI:\", doi_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i also check the already created `doi_abstracts.jsonl` file to see if there are empty abstracts (there are many)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_2 = \"10.1016/j.cell.2023.12.034\"\n",
    "\n",
    "url_test_2 = f\"https://api.crossref.org/works/{test_2}\"\n",
    "response_test_2 = requests.get(url_test_2)\n",
    "\n",
    "if response_test_2.status_code == 200:\n",
    "        data_test_2 = response_test_2.json()\n",
    "        with open(f\"response2_{test_2.replace('/', '.')}.json\", \"w\") as f:\n",
    "            json.dump(data_test_2, f, ensure_ascii=False, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the end, i manually searched and added the abstracts for the 9 papers that the api failed, and there are about 17 more that the api fetched empty abstracts."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
